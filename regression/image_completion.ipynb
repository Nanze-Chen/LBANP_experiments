{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/latent-bottlenecked-anp/regression\n"
     ]
    }
   ],
   "source": [
    "%cd /rds/user/fz287/hpc-work/MLMI4/latent-bottlenecked-anp/regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CelebA dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument dropout: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 785606\n",
      "\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.46it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 1 lr 5.000e-04 tar_ll 1.2075 loss -1.2075 (85.158 secs)\n",
      "100%|##########| 1827/1827 [01:24<00:00, 21.55it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 2 lr 4.999e-04 tar_ll 2.0804 loss -2.0804 (84.776 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.23it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 3 lr 4.997e-04 tar_ll 2.2247 loss -2.2247 (86.049 secs)\n",
      "100%|##########| 1827/1827 [01:24<00:00, 21.51it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 4 lr 4.995e-04 tar_ll 2.3544 loss -2.3544 (84.937 secs)\n",
      "100%|##########| 1827/1827 [01:24<00:00, 21.60it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 5 lr 4.992e-04 tar_ll 2.4093 loss -2.4093 (84.595 secs)\n",
      "100%|##########| 1827/1827 [01:22<00:00, 22.06it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 6 lr 4.989e-04 tar_ll 2.4410 loss -2.4410 (82.841 secs)\n",
      "100%|##########| 1827/1827 [01:21<00:00, 22.37it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 7 lr 4.985e-04 tar_ll 2.5131 loss -2.5131 (81.687 secs)\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.46it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 8 lr 4.980e-04 tar_ll 2.5198 loss -2.5198 (85.128 secs)\n",
      "100%|##########| 1827/1827 [01:24<00:00, 21.49it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 9 lr 4.975e-04 tar_ll 2.5779 loss -2.5779 (85.008 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.14it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 10 lr 4.969e-04 tar_ll 2.5878 loss -2.5878 (86.432 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating evaluation sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1248/1248 [00:01<00:00, 706.26it/s]\n",
      "100%|##########| 1248/1248 [00:12<00:00, 96.26it/s]\n",
      "lbanp:lbanp_celeba32_8 tar_ll 2.5854 loss -2.5854 (12.973 secs)\n",
      "\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.39it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 11 lr 4.963e-04 tar_ll 2.6032 loss -2.6032 (85.541 secs)\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.28it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 12 lr 4.956e-04 tar_ll 2.5976 loss -2.5976 (85.853 secs)\n",
      "100%|##########| 1827/1827 [01:21<00:00, 22.36it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 13 lr 4.948e-04 tar_ll 2.6820 loss -2.6820 (81.721 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.02it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 14 lr 4.940e-04 tar_ll 2.6330 loss -2.6330 (86.912 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.10it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 15 lr 4.931e-04 tar_ll 2.7260 loss -2.7260 (86.603 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.12it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 16 lr 4.921e-04 tar_ll 2.6891 loss -2.6891 (86.530 secs)\n",
      "100%|##########| 1827/1827 [01:23<00:00, 21.75it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 17 lr 4.911e-04 tar_ll 2.6793 loss -2.6793 (83.997 secs)\n",
      "100%|##########| 1827/1827 [01:21<00:00, 22.50it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 18 lr 4.901e-04 tar_ll 2.7717 loss -2.7717 (81.213 secs)\n",
      "100%|##########| 1827/1827 [01:22<00:00, 22.14it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 19 lr 4.889e-04 tar_ll 2.6887 loss -2.6887 (82.517 secs)\n",
      "100%|##########| 1827/1827 [01:21<00:00, 22.48it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 20 lr 4.878e-04 tar_ll 2.6771 loss -2.6771 (81.290 secs)\n",
      "100%|##########| 1248/1248 [00:12<00:00, 96.77it/s]\n",
      "lbanp:lbanp_celeba32_8 tar_ll 2.7210 loss -2.7210 (12.905 secs)\n",
      "\n",
      "100%|##########| 1827/1827 [01:23<00:00, 21.76it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 21 lr 4.865e-04 tar_ll 2.7745 loss -2.7745 (84.001 secs)\n",
      "100%|##########| 1827/1827 [01:22<00:00, 22.08it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 22 lr 4.852e-04 tar_ll 2.7796 loss -2.7796 (82.766 secs)\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.38it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 23 lr 4.839e-04 tar_ll 2.7280 loss -2.7280 (85.464 secs)\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.47it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 24 lr 4.824e-04 tar_ll 2.7742 loss -2.7742 (85.095 secs)\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.38it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 25 lr 4.810e-04 tar_ll 2.7763 loss -2.7763 (85.444 secs)\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.31it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 26 lr 4.794e-04 tar_ll 2.8246 loss -2.8246 (85.747 secs)\n",
      "100%|##########| 1827/1827 [01:24<00:00, 21.61it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 27 lr 4.779e-04 tar_ll 2.8105 loss -2.8105 (84.547 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.01it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 28 lr 4.762e-04 tar_ll 2.7751 loss -2.7751 (86.954 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.08it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 29 lr 4.745e-04 tar_ll 2.8233 loss -2.8233 (86.663 secs)\n",
      "100%|##########| 1827/1827 [01:23<00:00, 21.87it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 30 lr 4.728e-04 tar_ll 2.8374 loss -2.8374 (83.537 secs)\n",
      "100%|##########| 1248/1248 [00:12<00:00, 98.44it/s]\n",
      "lbanp:lbanp_celeba32_8 tar_ll 2.8204 loss -2.8204 (12.686 secs)\n",
      "\n",
      "100%|##########| 1827/1827 [01:21<00:00, 22.39it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 31 lr 4.709e-04 tar_ll 2.8252 loss -2.8252 (81.776 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.13it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 32 lr 4.691e-04 tar_ll 2.8583 loss -2.8583 (86.494 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.23it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 33 lr 4.672e-04 tar_ll 2.7560 loss -2.7560 (86.054 secs)\n",
      "100%|##########| 1827/1827 [01:26<00:00, 21.21it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 34 lr 4.652e-04 tar_ll 2.8610 loss -2.8610 (86.140 secs)\n",
      "100%|##########| 1827/1827 [02:09<00:00, 14.16it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 35 lr 4.632e-04 tar_ll 2.8370 loss -2.8370 (129.026 secs)\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.46it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 36 lr 4.611e-04 tar_ll 2.8301 loss -2.8301 (85.126 secs)\n",
      "100%|##########| 1827/1827 [01:25<00:00, 21.35it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 37 lr 4.590e-04 tar_ll 2.8786 loss -2.8786 (85.569 secs)\n",
      " 18%|█▊        | 37/200 [53:40<4:10:22, 92.16s/it]"
     ]
    }
   ],
   "source": [
    "# LBANP (8) -- celeba 32x32\n",
    "%run celeba.py --mode train --expid lbanp_celeba32_8 --model lbanp --num_latents 8 --resolution 32 --max_num_points 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (128) -- celeba 32x32\n",
    "%run celeba.py --mode train --expid lbanp_celeba32_128 --model lbanp --num_latents 128 --resolution 32 --max_num_points 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (8) -- celeba 64x64\n",
    "%run celeba.py --mode train --expid lbanp_celeba64_8 --model lbanp --num_latents 8 --resolution 64 --max_num_points 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (8) -- celeba 128x128\n",
    "%run celeba.py --mode train --expid lbanp_celeba128_8 --model lbanp --num_latents 8 --resolution 128 --max_num_points 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMNIST dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (8)\n",
    "%run emnist.py --mode train --expid lbanp_emnist_8 --model lbanp --num_latents 8 --resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/np_venv/lib64/python3.6/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "  0%|          | 0/240 [00:00<?, ?it/s]/rds/user/fz287/hpc-work/MLMI4/np_venv/lib64/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "100%|##########| 240/240 [00:12<00:00, 19.60it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 191 lr 2.494e-06 tar_ll 1.4418 loss -1.4418 (12.252 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 21.01it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 192 lr 1.971e-06 tar_ll 1.4023 loss -1.4023 (11.428 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.96it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 193 lr 1.510e-06 tar_ll 1.3990 loss -1.3990 (11.458 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.92it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 194 lr 1.110e-06 tar_ll 1.3750 loss -1.3750 (11.476 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.98it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 195 lr 7.707e-07 tar_ll 1.4285 loss -1.4285 (11.447 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.90it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 196 lr 4.933e-07 tar_ll 1.4042 loss -1.4042 (11.487 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.99it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 197 lr 2.775e-07 tar_ll 1.3876 loss -1.3876 (11.437 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.95it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 198 lr 1.234e-07 tar_ll 1.4299 loss -1.4299 (11.459 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.99it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 199 lr 3.084e-08 tar_ll 1.4102 loss -1.4102 (11.441 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.98it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 200 lr 0.000e+00 tar_ll 1.4285 loss -1.4285 (11.444 secs)\n",
      "100%|##########| 250/250 [00:02<00:00, 89.63it/s]\n",
      "lbanp:lbanp_emnist_128 0-10 tar_ll 1.3944 loss -1.3944 (2.795 secs)\n",
      "\n",
      "100%|##########| 250/250 [00:02<00:00, 90.14it/s]\n",
      "lbanp:lbanp_emnist_128 0-10 tar_ll 1.3944 loss -1.3944 (2.779 secs)\n",
      "lbanp:lbanp_emnist_128 0-10 tar_ll 1.3944 loss -1.3944 (2.779 secs)\n"
     ]
    }
   ],
   "source": [
    "# LBANP (128)\n",
    "%run emnist.py --mode train --expid lbanp_emnist_128 --model lbanp --num_latents 128 --resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
