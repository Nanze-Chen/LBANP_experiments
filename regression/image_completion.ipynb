{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/latent-bottlenecked-anp/regression\n"
     ]
    }
   ],
   "source": [
    "%cd /rds/user/fz287/hpc-work/MLMI4/latent-bottlenecked-anp/regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CelebA dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument dropout: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/170 [00:00<?, ?it/s]/rds/user/fz287/hpc-work/MLMI4/np_venv/lib64/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "100%|##########| 1827/1827 [01:35<00:00, 19.21it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 31 lr 4.709e-04 tar_ll 2.8544 loss -2.8544 (95.114 secs)\n",
      "100%|##########| 1827/1827 [01:38<00:00, 18.52it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 32 lr 4.691e-04 tar_ll 2.8466 loss -2.8466 (98.665 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.30it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 33 lr 4.672e-04 tar_ll 2.8851 loss -2.8851 (99.858 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.29it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 34 lr 4.652e-04 tar_ll 2.8459 loss -2.8459 (99.906 secs)\n",
      "100%|##########| 1827/1827 [01:38<00:00, 18.63it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 35 lr 4.632e-04 tar_ll 2.8728 loss -2.8728 (98.057 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.43it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 36 lr 4.611e-04 tar_ll 2.8216 loss -2.8216 (99.132 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.45it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 37 lr 4.590e-04 tar_ll 2.8890 loss -2.8890 (99.041 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.28it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 38 lr 4.568e-04 tar_ll 2.8926 loss -2.8926 (99.976 secs)\n",
      "100%|##########| 1827/1827 [01:40<00:00, 18.26it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 39 lr 4.545e-04 tar_ll 2.8576 loss -2.8576 (100.067 secs)\n",
      "100%|##########| 1827/1827 [01:40<00:00, 18.21it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 40 lr 4.523e-04 tar_ll 2.8679 loss -2.8679 (100.340 secs)\n",
      "100%|##########| 1248/1248 [00:28<00:00, 44.35it/s]\n",
      "lbanp:lbanp_celeba32_8 tar_ll 2.7801 loss -2.7801 (28.146 secs)\n",
      "\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.35it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 41 lr 4.499e-04 tar_ll 2.9422 loss -2.9422 (99.640 secs)\n",
      "100%|##########| 1827/1827 [01:40<00:00, 18.26it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 42 lr 4.475e-04 tar_ll 2.9453 loss -2.9453 (100.067 secs)\n",
      "100%|##########| 1827/1827 [01:38<00:00, 18.58it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 43 lr 4.451e-04 tar_ll 2.9644 loss -2.9644 (98.326 secs)\n",
      "100%|##########| 1827/1827 [01:40<00:00, 18.27it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 44 lr 4.426e-04 tar_ll 2.9451 loss -2.9451 (100.025 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.27it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 45 lr 4.401e-04 tar_ll 2.9117 loss -2.9117 (99.983 secs)\n",
      "100%|##########| 1827/1827 [01:33<00:00, 19.54it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 46 lr 4.375e-04 tar_ll 2.8803 loss -2.8803 (93.490 secs)\n",
      "100%|##########| 1827/1827 [01:38<00:00, 18.64it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 47 lr 4.349e-04 tar_ll 2.8979 loss -2.8979 (98.040 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.36it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 48 lr 4.322e-04 tar_ll 2.9556 loss -2.9556 (99.499 secs)\n",
      "100%|##########| 1827/1827 [01:38<00:00, 18.54it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 49 lr 4.295e-04 tar_ll 2.9815 loss -2.9815 (98.547 secs)\n",
      "100%|##########| 1827/1827 [02:09<00:00, 14.08it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 50 lr 4.268e-04 tar_ll 2.9280 loss -2.9280 (129.777 secs)\n",
      "100%|##########| 1248/1248 [00:28<00:00, 43.88it/s]\n",
      "lbanp:lbanp_celeba32_8 tar_ll 2.8898 loss -2.8898 (28.449 secs)\n",
      "\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.27it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 51 lr 4.240e-04 tar_ll 2.9272 loss -2.9272 (100.123 secs)\n",
      "100%|##########| 1827/1827 [01:40<00:00, 18.26it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 52 lr 4.211e-04 tar_ll 2.9796 loss -2.9796 (100.091 secs)\n",
      "100%|##########| 1827/1827 [01:40<00:00, 18.24it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 53 lr 4.183e-04 tar_ll 2.9205 loss -2.9205 (100.177 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.31it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 54 lr 4.153e-04 tar_ll 2.9400 loss -2.9400 (99.777 secs)\n",
      "100%|##########| 1827/1827 [01:40<00:00, 18.24it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 55 lr 4.124e-04 tar_ll 2.9802 loss -2.9802 (100.166 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.29it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 56 lr 4.094e-04 tar_ll 2.9724 loss -2.9724 (99.878 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.29it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 57 lr 4.063e-04 tar_ll 2.9559 loss -2.9559 (99.917 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.33it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 58 lr 4.032e-04 tar_ll 3.0251 loss -3.0251 (99.657 secs)\n",
      "100%|##########| 1827/1827 [01:40<00:00, 18.22it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 59 lr 4.001e-04 tar_ll 3.0032 loss -3.0032 (100.274 secs)\n",
      "100%|##########| 1827/1827 [01:40<00:00, 18.16it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 60 lr 3.969e-04 tar_ll 3.0330 loss -3.0330 (100.624 secs)\n",
      "100%|##########| 1248/1248 [00:20<00:00, 61.84it/s]\n",
      "lbanp:lbanp_celeba32_8 tar_ll 3.0251 loss -3.0251 (20.188 secs)\n",
      "\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.37it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 61 lr 3.938e-04 tar_ll 3.0736 loss -3.0736 (99.531 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.33it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 62 lr 3.905e-04 tar_ll 3.0697 loss -3.0697 (99.662 secs)\n",
      "100%|##########| 1827/1827 [01:39<00:00, 18.35it/s]\n",
      "lbanp:lbanp_celeba32_8 epoch 63 lr 3.873e-04 tar_ll 3.0707 loss -3.0707 (99.572 secs)\n",
      " 19%|█▉        | 33/170 [56:24<3:53:00, 102.05s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://gpu-q-1:8888/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# LBANP (8) -- celeba 32x32\n",
    "%run celeba.py --mode train --expid lbanp_celeba32_8 --model lbanp --num_latents 8 --resolution 32 --max_num_points 200 --resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (128) -- celeba 32x32\n",
    "%run celeba.py --mode train --expid lbanp_celeba32_128 --model lbanp --num_latents 128 --resolution 32 --max_num_points 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (8) -- celeba 64x64\n",
    "%run celeba.py --mode train --expid lbanp_celeba64_8 --model lbanp --num_latents 8 --resolution 64 --max_num_points 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (8) -- celeba 128x128\n",
    "%run celeba.py --mode train --expid lbanp_celeba128_8 --model lbanp --num_latents 8 --resolution 128 --max_num_points 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMNIST dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (8)\n",
    "%run emnist.py --mode train --expid lbanp_emnist_8 --model lbanp --num_latents 8 --resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/np_venv/lib64/python3.6/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "  0%|          | 0/240 [00:00<?, ?it/s]/rds/user/fz287/hpc-work/MLMI4/np_venv/lib64/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "100%|##########| 240/240 [00:12<00:00, 19.60it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 191 lr 2.494e-06 tar_ll 1.4418 loss -1.4418 (12.252 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 21.01it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 192 lr 1.971e-06 tar_ll 1.4023 loss -1.4023 (11.428 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.96it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 193 lr 1.510e-06 tar_ll 1.3990 loss -1.3990 (11.458 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.92it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 194 lr 1.110e-06 tar_ll 1.3750 loss -1.3750 (11.476 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.98it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 195 lr 7.707e-07 tar_ll 1.4285 loss -1.4285 (11.447 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.90it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 196 lr 4.933e-07 tar_ll 1.4042 loss -1.4042 (11.487 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.99it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 197 lr 2.775e-07 tar_ll 1.3876 loss -1.3876 (11.437 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.95it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 198 lr 1.234e-07 tar_ll 1.4299 loss -1.4299 (11.459 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.99it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 199 lr 3.084e-08 tar_ll 1.4102 loss -1.4102 (11.441 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.98it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 200 lr 0.000e+00 tar_ll 1.4285 loss -1.4285 (11.444 secs)\n",
      "100%|##########| 250/250 [00:02<00:00, 89.63it/s]\n",
      "lbanp:lbanp_emnist_128 0-10 tar_ll 1.3944 loss -1.3944 (2.795 secs)\n",
      "\n",
      "100%|##########| 250/250 [00:02<00:00, 90.14it/s]\n",
      "lbanp:lbanp_emnist_128 0-10 tar_ll 1.3944 loss -1.3944 (2.779 secs)\n",
      "lbanp:lbanp_emnist_128 0-10 tar_ll 1.3944 loss -1.3944 (2.779 secs)\n"
     ]
    }
   ],
   "source": [
    "# LBANP (128)\n",
    "%run emnist.py --mode train --expid lbanp_emnist_128 --model lbanp --num_latents 128 --resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
