{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/latent-bottlenecked-anp/regression\n"
     ]
    }
   ],
   "source": [
    "%cd /rds/user/fz287/hpc-work/MLMI4/latent-bottlenecked-anp/regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CelebA dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (8) -- celeba 32x32\n",
    "%run celeba.py --mode train --expid lbanp_celeba32_8 --model lbanp --num_latents 8 --resolution 32 --max_num_points 200 --resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (128) -- celeba 32x32\n",
    "%run celeba.py --mode train --expid lbanp_celeba32_128 --model lbanp --num_latents 128 --resolution 32 --max_num_points 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument dropout: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 785606\n",
      "\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]/rds/user/fz287/hpc-work/MLMI4/np_venv/lib64/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "100%|##########| 1827/1827 [02:32<00:00, 12.01it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 1 lr 5.000e-04 tar_ll 1.2860 loss -1.2860 (152.199 secs)\n",
      "100%|##########| 1827/1827 [02:29<00:00, 12.21it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 2 lr 4.999e-04 tar_ll 2.4088 loss -2.4088 (149.623 secs)\n",
      "100%|##########| 1827/1827 [02:30<00:00, 12.12it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 3 lr 4.997e-04 tar_ll 2.6901 loss -2.6901 (150.764 secs)\n",
      "100%|##########| 1827/1827 [02:30<00:00, 12.15it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 4 lr 4.995e-04 tar_ll 2.8880 loss -2.8880 (150.435 secs)\n",
      "100%|##########| 1827/1827 [02:31<00:00, 12.07it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 5 lr 4.992e-04 tar_ll 2.9661 loss -2.9661 (151.337 secs)\n",
      "100%|##########| 1827/1827 [02:30<00:00, 12.14it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 6 lr 4.989e-04 tar_ll 3.0127 loss -3.0127 (150.445 secs)\n",
      "100%|##########| 1827/1827 [02:22<00:00, 12.86it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 7 lr 4.985e-04 tar_ll 3.0588 loss -3.0588 (142.060 secs)\n",
      "100%|##########| 1827/1827 [02:29<00:00, 12.19it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 8 lr 4.980e-04 tar_ll 3.1489 loss -3.1489 (149.899 secs)\n",
      "100%|##########| 1827/1827 [02:29<00:00, 12.19it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 9 lr 4.975e-04 tar_ll 3.2194 loss -3.2194 (149.908 secs)\n",
      "100%|##########| 1827/1827 [02:31<00:00, 12.10it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 10 lr 4.969e-04 tar_ll 3.2490 loss -3.2490 (151.034 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating evaluation sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1248/1248 [00:02<00:00, 529.00it/s]\n",
      "100%|##########| 1248/1248 [00:18<00:00, 66.96it/s]\n",
      "lbanp:lbanp_celeba64_8 tar_ll 3.3571 loss -3.3571 (18.647 secs)\n",
      "\n",
      "100%|##########| 1827/1827 [02:31<00:00, 12.09it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 11 lr 4.963e-04 tar_ll 3.2875 loss -3.2875 (151.211 secs)\n",
      "100%|##########| 1827/1827 [02:31<00:00, 12.09it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 12 lr 4.956e-04 tar_ll 3.2730 loss -3.2730 (151.160 secs)\n",
      "100%|##########| 1827/1827 [03:03<00:00,  9.95it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 13 lr 4.948e-04 tar_ll 3.3431 loss -3.3431 (183.692 secs)\n",
      "100%|##########| 1827/1827 [02:23<00:00, 12.75it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 14 lr 4.940e-04 tar_ll 3.3605 loss -3.3605 (143.287 secs)\n",
      "100%|##########| 1827/1827 [02:31<00:00, 12.03it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 15 lr 4.931e-04 tar_ll 3.3537 loss -3.3537 (151.902 secs)\n",
      "100%|##########| 1827/1827 [02:31<00:00, 12.10it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 16 lr 4.921e-04 tar_ll 3.4381 loss -3.4381 (151.048 secs)\n",
      "100%|##########| 1827/1827 [02:29<00:00, 12.21it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 17 lr 4.911e-04 tar_ll 3.3451 loss -3.3451 (149.641 secs)\n",
      "100%|##########| 1827/1827 [02:32<00:00, 12.00it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 18 lr 4.901e-04 tar_ll 3.4505 loss -3.4505 (152.248 secs)\n",
      "100%|##########| 1827/1827 [02:30<00:00, 12.18it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 19 lr 4.889e-04 tar_ll 3.4205 loss -3.4205 (150.054 secs)\n",
      "100%|##########| 1827/1827 [02:28<00:00, 12.28it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 20 lr 4.878e-04 tar_ll 3.4959 loss -3.4959 (148.821 secs)\n",
      "100%|##########| 1248/1248 [00:16<00:00, 75.71it/s]\n",
      "lbanp:lbanp_celeba64_8 tar_ll 3.4423 loss -3.4423 (16.492 secs)\n",
      "\n",
      "100%|##########| 1827/1827 [02:31<00:00, 12.09it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 21 lr 4.865e-04 tar_ll 3.4848 loss -3.4848 (151.187 secs)\n",
      "100%|##########| 1827/1827 [02:30<00:00, 12.10it/s]\n",
      "lbanp:lbanp_celeba64_8 epoch 22 lr 4.852e-04 tar_ll 3.5125 loss -3.5125 (150.971 secs)\n",
      " 11%|â–ˆ         | 22/200 [56:13<7:35:22, 153.49s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://gpu-q-1:8888/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# LBANP (8) -- celeba 64x64\n",
    "%run celeba.py --mode train --expid lbanp_celeba64_8 --model lbanp --num_latents 8 --resolution 64 --max_num_points 800 --resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (8) -- celeba 128x128\n",
    "%run celeba.py --mode train --expid lbanp_celeba128_8 --model lbanp --num_latents 8 --resolution 128 --max_num_points 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMNIST dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBANP (8)\n",
    "%run emnist.py --mode train --expid lbanp_emnist_8 --model lbanp --num_latents 8 --resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/np_venv/lib64/python3.6/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "  0%|          | 0/240 [00:00<?, ?it/s]/rds/user/fz287/hpc-work/MLMI4/np_venv/lib64/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "100%|##########| 240/240 [00:12<00:00, 19.60it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 191 lr 2.494e-06 tar_ll 1.4418 loss -1.4418 (12.252 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 21.01it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 192 lr 1.971e-06 tar_ll 1.4023 loss -1.4023 (11.428 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.96it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 193 lr 1.510e-06 tar_ll 1.3990 loss -1.3990 (11.458 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.92it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 194 lr 1.110e-06 tar_ll 1.3750 loss -1.3750 (11.476 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.98it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 195 lr 7.707e-07 tar_ll 1.4285 loss -1.4285 (11.447 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.90it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 196 lr 4.933e-07 tar_ll 1.4042 loss -1.4042 (11.487 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.99it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 197 lr 2.775e-07 tar_ll 1.3876 loss -1.3876 (11.437 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.95it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 198 lr 1.234e-07 tar_ll 1.4299 loss -1.4299 (11.459 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.99it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 199 lr 3.084e-08 tar_ll 1.4102 loss -1.4102 (11.441 secs)\n",
      "100%|##########| 240/240 [00:11<00:00, 20.98it/s]\n",
      "lbanp:lbanp_emnist_128 epoch 200 lr 0.000e+00 tar_ll 1.4285 loss -1.4285 (11.444 secs)\n",
      "100%|##########| 250/250 [00:02<00:00, 89.63it/s]\n",
      "lbanp:lbanp_emnist_128 0-10 tar_ll 1.3944 loss -1.3944 (2.795 secs)\n",
      "\n",
      "100%|##########| 250/250 [00:02<00:00, 90.14it/s]\n",
      "lbanp:lbanp_emnist_128 0-10 tar_ll 1.3944 loss -1.3944 (2.779 secs)\n",
      "lbanp:lbanp_emnist_128 0-10 tar_ll 1.3944 loss -1.3944 (2.779 secs)\n"
     ]
    }
   ],
   "source": [
    "# LBANP (128)\n",
    "%run emnist.py --mode train --expid lbanp_emnist_128 --model lbanp --num_latents 128 --resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
